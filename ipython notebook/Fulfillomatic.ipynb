{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fulfillomatic\n",
    "\n",
    "##### Adriana Souza, Roger Filmyer\n",
    "\n",
    "##### This notebook will be finished/cleaned by Thursday, Dec 6th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLG](http://www.pngall.com/wp-content/uploads/2016/07/Meditation-Transparent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the file to use\n",
    "file = 'training/quotes.txt'\n",
    "\n",
    "# Storing quotes from file in a list\n",
    "with open(file) as opened_file: \n",
    "    lists = opened_file.read().splitlines()\n",
    "    quotes = []\n",
    "    for line in lists:\n",
    "        quotes.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0: Uniform Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yourself lost enough be Some even , to flamboyantly Do that not .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize\n",
    "tokenized_corpus = []\n",
    "for quote in quotes:\n",
    "    tokenized_quote = nltk.tokenize.word_tokenize(quote)\n",
    "    tagged_quote = nltk.pos_tag(tokenized_quote)\n",
    "    tokenized_corpus.append(tagged_quote)\n",
    "\n",
    "# Set up the language model\n",
    "parts_of_speech = defaultdict(list)\n",
    "sentence_structures = []\n",
    "for quote in tokenized_corpus:\n",
    "    sentence_structure = []\n",
    "    for word, pos in quote:\n",
    "        parts_of_speech[pos].append(word)\n",
    "        sentence_structure.append(pos)\n",
    "    sentence_structures.append(sentence_structure)\n",
    "\n",
    "# Generate an example sentence\n",
    "def get_mindful_v0() -> str:\n",
    "    \"\"\"\n",
    "    Generate an inspirational sentence. \n",
    "    \n",
    "    Ensure that you are in the proper state of mind before running. ॐ\n",
    "    \"\"\"\n",
    "    sentence_skeleton = random.choice(sentence_structures)\n",
    "\n",
    "    reconstituted_sentence = []\n",
    "    for part_of_speech in sentence_skeleton:\n",
    "        new_word = random.choice(parts_of_speech[part_of_speech])\n",
    "        reconstituted_sentence.append(new_word)\n",
    "\n",
    "    return \" \".join(reconstituted_sentence)\n",
    "\n",
    "# Output\n",
    "get_mindful_v0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 0 results\n",
    "\n",
    "* your ready Speak begins when you can hear you not and never .\n",
    "* in I think busy forwards of coffee , forever it . in you will live aware library you , make your education .\n",
    "* without t denies my anything that bulk , yourself can once call You .\n",
    "* as I are dreams to don grief , never the sun is to tolerate able you .\n",
    "* all valuable choice is than the painful comfort , it can keep imprisoned believe only not that you ’ you ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1: Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that worked great. Maybe some context _would_ be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning list into string\n",
    "corpus = \"\"\n",
    "string\n",
    "for word in quotes:\n",
    "    word = word.lower()\n",
    "    word = word.replace('.', ' END ')\n",
    "    table = str.maketrans('','', string.punctuation + '…”“–')      # Remove punctuation\n",
    "    word = word.translate(table)\n",
    "    corpus = corpus + word  \n",
    "    \n",
    "def tokenize(input_string):\n",
    "    return input_string.split()\n",
    "\n",
    "def get_bigrams(corpus):\n",
    "    corpus_fd_unigram = nltk.FreqDist(tokenize(corpus))\n",
    "    total = sum([1 + i[1] for i in corpus_fd_unigram.items()])\n",
    "    bigrams = nltk.bigrams(['END'] + tokenize(corpus))\n",
    "    bigrams_fd = nltk.FreqDist(bigrams)\n",
    "    results = {}\n",
    "    for bigram, bigram_frequency in bigrams_fd.items():\n",
    "        first_word, second_word = bigram\n",
    "        probability = (bigram_frequency / corpus_fd_unigram[first_word])    \n",
    "        results[bigram] = probability\n",
    "    return results\n",
    "\n",
    "#bigrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversity reveals the shore but we cannot escape necessities but we are not the educated are falling into it right door you stand for what lies behind me there is no more good you get people i can make friends.\n",
      "People you only be.\n",
      "Know failure is not you will transform him his enemies try to exist only the fire and even destroy this body but rather the ones life the overcoming of states we are those who wander are looking for what lies behind me burned brighter than the present.\n",
      "I may not stop one of his woes.\n",
      "To your hope is right.\n"
     ]
    }
   ],
   "source": [
    "bigram_model = get_bigrams(corpus)\n",
    "\n",
    "def get_mindful_v1():\n",
    "    \"\"\"\n",
    "    You must only concentrate on the next step, the next breath, \n",
    "    the next stroke of the broom, and the next, and the next. Nothing else.\n",
    "    ॐ\n",
    "    \n",
    "    (Bigram Model)\n",
    "    \"\"\"\n",
    "    words_in_sentence = ['END']\n",
    "    second_word = None\n",
    "    while second_word != 'END':\n",
    "        first_word = words_in_sentence[-1]\n",
    "        matching_bigrams = [bigram for bigram in bigram_model.keys() if bigram[0] == first_word]\n",
    "        bigram_probabilities = [bigram_model[bigram] for bigram in matching_bigrams]\n",
    "        total_probability = sum(bigram_probabilities)\n",
    "        second_word = np.random.choice(\n",
    "                        a=[second for first, second in matching_bigrams],\n",
    "                        p=[p for p in bigram_probabilities])\n",
    "        words_in_sentence.append(second_word)\n",
    "    words_in_sentence = words_in_sentence[1:-1]\n",
    "    # capitalize first letter of first word\n",
    "    if len(words_in_sentence) > 0:\n",
    "        first_word = words_in_sentence[0]\n",
    "        first_word = first_word[0].upper() + first_word[1:]\n",
    "        words_in_sentence[0] = first_word\n",
    "        sentence = \" \".join(words_in_sentence) + '.'\n",
    "    else:\n",
    "        sentence = get_mindful_v1()\n",
    "    return sentence\n",
    "        \n",
    "# Print it a\n",
    "def repeat(times, f):\n",
    "    for i in range(times): f()\n",
    "        \n",
    "def do_v1():\n",
    "    print(get_mindful_v1())\n",
    "    \n",
    "repeat(5, do_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1 results\n",
    "\n",
    "* Just do it.\n",
    "* In my friends you can get the fire you grow from it should scare you do drunk.\n",
    "* You.\n",
    "* I believe in the least for anything i believe in god from a man to exist.\n",
    "* Dont bother just take rest is too little one that you better.\n",
    "* If you can not what we know what you will remain constant.\n",
    "* What we are travelling more difficult than to forget is no greatness.\n",
    "* Anything you look for what you do not being yourself.\n",
    "* Let the wilderness of all else is still looking for us entirely happy because i told dismiss that can do something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: Trigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(...) We don't want trigrams that span from the end of one file to the next. Such trigrams do not represent tokens that could follow each other in a text-- they are completely accidental.\n",
    "\n",
    "(...)\n",
    "\n",
    "We added double end tokens for the trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding extra END tokens\n",
    "def add_extra_end_token(tokenized_document):\n",
    "    new_document = []\n",
    "    for token in tokenized_document:\n",
    "        new_document.append(token)\n",
    "        if token == \"END\":\n",
    "            new_document.append(\"END\")\n",
    "    return new_document\n",
    "\n",
    "def get_trigrams(document):\n",
    "    corpus = tokenize(document)\n",
    "    corpus = add_extra_end_token(corpus)\n",
    "    corpus_fd_bigram = nltk.FreqDist(nltk.bigrams([\"END\"] + corpus))\n",
    "    trigrams = nltk.trigrams([\"END\", \"END\"] + corpus)\n",
    "    trigrams_fd = nltk.FreqDist(trigrams)\n",
    "    results = {}\n",
    "    for trigram, trigram_frequency in trigrams_fd.items():\n",
    "        first_word, second_word, third_word = trigram\n",
    "        probability = (trigram_frequency) / (corpus_fd_bigram[(first_word, second_word)])\n",
    "        results[trigram] = probability\n",
    "    return results\n",
    "\n",
    "#get_trigrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aim of art is to keep company only with people who uplift you whose presence calls forth your best.\n",
      "Nothing is more difficult than to need and not as a book over and over again there is.\n",
      "I am changing myself.\n",
      "If you do not know yourselves then you are still looking for that one person who will risk going too far can possibly find out why.\n",
      "If you have not yet see the stars is ambitious.\n",
      "That it is literally true that you have to be what you read when you die you will have treasure in heaven.\n",
      "What lies before us are looking at the broken places.\n",
      "Be sure you put your feet in the darkness can you find truth you will liberate the minds of men.\n",
      "Real education must ultimately be limited to men who insist on knowing the rest said was right.\n",
      "Happiness is beneficial for the moon and if you grow in awareness you will always find happiness.\n"
     ]
    }
   ],
   "source": [
    "trigram_model = get_trigrams(corpus)\n",
    "\n",
    "def get_sentence_with_ngram_model(num_words, model):\n",
    "    words_in_sentence = ['END' for i in range(0, num_words - 1)] # pad the start of the sentence with 'END' tokens\n",
    "    final_word = None\n",
    "    while final_word != 'END':        \n",
    "        initial_n_gram_words = words_in_sentence[-(num_words - 1):]\n",
    "        matching_n_gram_keys = []\n",
    "        for n_gram in model.keys():\n",
    "            words_to_match = zip(n_gram, initial_n_gram_words)\n",
    "            if all(a == b for a, b in words_to_match):\n",
    "                matching_n_gram_keys.append(n_gram)        \n",
    "        n_gram_probabilities = [model[n_gram] for n_gram in matching_n_gram_keys]        \n",
    "        total_probability = sum(n_gram_probabilities)                \n",
    "        final_word = np.random.choice(\n",
    "                        a=[n_gram[-1] for n_gram in matching_n_gram_keys],\n",
    "                        p=[p for p in n_gram_probabilities])\n",
    "        words_in_sentence.append(final_word)\n",
    "    words_in_sentence = words_in_sentence[(num_words - 1): -1]\n",
    "    # capitalize first letter of first word\n",
    "    if len(words_in_sentence) > 0:\n",
    "        first_word = words_in_sentence[0]\n",
    "        first_word = first_word[0].upper() + first_word[1:]\n",
    "        words_in_sentence[0] = first_word\n",
    "        sentence = \" \".join(words_in_sentence) + '.'\n",
    "    else:\n",
    "        sentence = get_sentence_with_ngram_model(num_words, model)\n",
    "    return sentence\n",
    "\n",
    "def get_mindful_v2():\n",
    "    \"\"\"\n",
    "    Three things cannot long be hidden: the sun, the moon, and the truth. ॐ\n",
    "    \n",
    "    (Trigram Model)\n",
    "    \"\"\"\n",
    "    sentence = \"\"\n",
    "    while len(sentence.split()) < 4:\n",
    "        sentence = get_sentence_with_ngram_model(3, trigram_model)\n",
    "    return sentence\n",
    "    \n",
    "        \n",
    "# Print a bunch of generated sentences\n",
    "def repeat(times, f):\n",
    "    for i in range(times): f()\n",
    "        \n",
    "def do_v1():\n",
    "    print(get_mindful_v2())\n",
    "    \n",
    "repeat(10, do_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Take:\n",
    "\n",
    "        The world is full of magic things, patiently waiting for our senses *to grow* sharper. \n",
    "\n",
    "And:\n",
    "\n",
    "        It takes courage *to grow* up and become who you really are. \n",
    "\n",
    "Get:\n",
    "\n",
    "            It takes courage *to grow* sharper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLG](https://supportivedivorcesolutions.com/wp-content/uploads/2017/03/iStock-468140568.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
