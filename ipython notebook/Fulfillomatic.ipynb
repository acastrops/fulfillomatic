{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fulfillomatic\n",
    "\n",
    "##### Adriana Souza, Roger Filmyer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLG](http://www.pngall.com/wp-content/uploads/2016/07/Meditation-Transparent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, sentence generation can either be really easy or really hard depending on the quality of output you'd like. NLG (Natural Language Generation), mainly deals with generating text that describes a set of data. In our case, we are taking a training corpus of quotes and generating our own context-free-grammar sentences.\n",
    "\n",
    "One common approach is using n-gram based sentence generators. Here, the output is sometimes riddled with nonsense that is often ungrammatical since these just chain together of words that tend to appear in sequence. However, word on the street is that 3-4-gram generators look quite okay sometimes.\n",
    "\n",
    "In this notebook, we tried 4 approaches: \n",
    "\n",
    "1. Unweighted Parts-of-Speech (Fulfillomatic v0)\n",
    "2. Bigram Model (Fulfillomatic v1)\n",
    "3. Trigram Model (Fulfillomatic v2)\n",
    "4. LSTM (Fulfillomatic v3)\n",
    "\n",
    "Results varied and, as expected, improved steadily as we moved from v0 to v2, but the quality of the output changed dramatically with the LSTM. We believe this might be because of the strange structure of our training data: inspirational/zen quotes are single sentences that almost have to be treated as individual documents. Single sentences have significantly less words than more common corpora which usually include one or more full bodies of text. \n",
    "\n",
    "Future work includes implementing some kind of subject verb agreement, some smarter padding and input sequences for the LSTM, and using Markov chains to generate sentences. This could be done using a transition matrix that says how likely it is to transition between every every part-of-speech.\n",
    "\n",
    "Ultimately, the most important thing is to have inner peace. \n",
    "\n",
    "Namaste ॐ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the file to use\n",
    "#file = 'training/inspirational_quotes.txt'\n",
    "#file = 'training/nietzsche_quotes.txt'\n",
    "file = 'training/zen_quotes.txt'\n",
    "#file = 'training/everything.txt'  # Worse quote results due to different styles being mixed together.\n",
    "\n",
    "# Storing quotes from file in a list\n",
    "with open(file, encoding='utf-8') as opened_file: \n",
    "    lists = opened_file.read().splitlines()\n",
    "    quotes = []\n",
    "    for line in lists:\n",
    "        quotes.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0: Unweighted Parts-of-Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we tried..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenized_corpus = []\n",
    "for quote in quotes:\n",
    "    tokenized_quote = nltk.tokenize.word_tokenize(quote)\n",
    "    tagged_quote = nltk.pos_tag(tokenized_quote)\n",
    "    tokenized_corpus.append(tagged_quote)\n",
    "\n",
    "# Set up the language \"model\"\n",
    "parts_of_speech = defaultdict(list)\n",
    "sentence_structures = []\n",
    "for quote in tokenized_corpus:\n",
    "    sentence_structure = []\n",
    "    for word, pos in quote:\n",
    "        parts_of_speech[pos].append(word)\n",
    "        sentence_structure.append(pos)\n",
    "    sentence_structures.append(sentence_structure)\n",
    "\n",
    "# Generate an example sentence\n",
    "# get_mindful_v0()\n",
    "def chaos():\n",
    "    \"\"\"\n",
    "    Generate an inspirational sentence. \n",
    "    \n",
    "    Ensure that you are in the proper state of mind before running. ॐ\n",
    "    \"\"\"\n",
    "    sentence_skeleton = random.choice(sentence_structures)\n",
    "    reconstituted_sentence = []\n",
    "    for part_of_speech in sentence_skeleton:\n",
    "        new_word = random.choice(parts_of_speech[part_of_speech])\n",
    "        reconstituted_sentence.append(new_word)\n",
    "    return \" \".join(reconstituted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of we exist motionless , buddhahood up . with yourself are possible , awaken out . you have the opinion on the bottle with rebellion and eating .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output\n",
    "chaos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 0 - Chaos: Results\n",
    "\n",
    "* your ready Speak begins when you can hear you not and never .\n",
    "* in I think busy forwards of coffee , forever it . in you will live aware library you , make your education .\n",
    "* the poison of purpose is to see nowhere a interesting majority because roads , and your able atom .\n",
    "* without t denies my anything that bulk , yourself can once call You .\n",
    "* as I are dreams to don grief , never the sun is to tolerate able you .\n",
    "* all valuable choice is than the painful comfort , it can keep imprisoned believe only not that you ’ you ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next:\n",
    "\n",
    "We see we need to do a lot of things, most of which we should've done even before we started (like lowercasing, removing punctuation, taking care of contractions). It seems that just assuming words would have a uniform distribution if we know the input is some sort of \"quote\"-esque type sentence wasn't enough. Since we kept our quotes separate and they aren't particularly long sentences, let's start with a bigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1: Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that worked great. Maybe some context _would_ be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning list into string\n",
    "corpus = \"\"\n",
    "for word in quotes:\n",
    "    # Lowercasing\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Adding end tokens to mark the end of quotes\n",
    "    word = word.replace('.', ' END ')   \n",
    "    \n",
    "    # Remove punctuation\n",
    "    table = str.maketrans('','', string.punctuation + '…”“–')      \n",
    "    word = word.translate(table)\n",
    "    \n",
    "    # Adding cleaned text to corpus\n",
    "    corpus = corpus + word  \n",
    "\n",
    "# Tokenizing\n",
    "def tokenize(input_string):\n",
    "    return input_string.split()\n",
    "\n",
    "# Getting bigram model\n",
    "def get_bigrams(corpus):\n",
    "    corpus_fd_unigram = nltk.FreqDist(tokenize(corpus))\n",
    "    bigrams = nltk.bigrams(['END'] + tokenize(corpus))\n",
    "    bigrams_fd = nltk.FreqDist(bigrams)\n",
    "    results = {}\n",
    "    for bigram, bigram_frequency in bigrams_fd.items():\n",
    "        first_word, second_word = bigram\n",
    "        probability = (bigram_frequency / corpus_fd_unigram[first_word])    \n",
    "        results[bigram] = probability\n",
    "    return results\n",
    "\n",
    "bigram_model = get_bigrams(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New version \n",
    "\n",
    "Below, we use a bigram model and also take some care in structuring how the sentence will come out. We make sure that our quote starts with a bigram of the form `[END, word]` and ends with a bigram of the form `[word, END]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to get an n-gram model\n",
    "def get_sentence_with_ngram_model(num_words, model):\n",
    "    words_in_sentence = ['END' for i in range(0, num_words - 1)] # Pad the start of the sentence with 'END' tokens\n",
    "    final_word = None\n",
    "    \n",
    "    while final_word != 'END':        \n",
    "        initial_n_gram_words = words_in_sentence[-(num_words - 1):]\n",
    "        matching_n_gram_keys = []\n",
    "        \n",
    "        #Get probabilites\n",
    "        for n_gram in model.keys():\n",
    "            words_to_match = zip(n_gram, initial_n_gram_words)\n",
    "            if all(a == b for a, b in words_to_match):\n",
    "                matching_n_gram_keys.append(n_gram) \n",
    "                \n",
    "        # Pick probabilities        \n",
    "        n_gram_probabilities = [model[n_gram] for n_gram in matching_n_gram_keys]        \n",
    "        total_probability = sum(n_gram_probabilities)                \n",
    "        final_word = np.random.choice(\n",
    "                        a=[n_gram[-1] for n_gram in matching_n_gram_keys],\n",
    "                        p=[p for p in n_gram_probabilities])\n",
    "        words_in_sentence.append(final_word)\n",
    "        \n",
    "    words_in_sentence = words_in_sentence[(num_words - 1): -1]\n",
    "    \n",
    "    # Capitalize first letter of first word\n",
    "    if len(words_in_sentence) > 0:\n",
    "        first_word = words_in_sentence[0]\n",
    "        first_word = first_word[0].upper() + first_word[1:]\n",
    "        words_in_sentence[0] = first_word\n",
    "        sentence = \" \".join(words_in_sentence) + '.'\n",
    "    else:\n",
    "        sentence = get_sentence_with_ngram_model(num_words, model)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it with a bigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 of Fulfillomatic\n",
    "def duality():    \n",
    "    \"\"\"\n",
    "    You must only concentrate on the next step, the next breath, \n",
    "    the next stroke of the broom, and the next, and the next. Nothing else.\n",
    "    ॐ\n",
    "    \n",
    "    (Bigram Model)\n",
    "    \"\"\"    \n",
    "    sentence = \"\"\n",
    "    while len(sentence.split()) < 4:\n",
    "        sentence = get_sentence_with_ngram_model(2, bigram_model)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By what our task must be serene in the future is not happening in its very nature of the night rain just let its searing power.\n",
      "The unreality of zen merely points.\n",
      "It is only because he takes a cult.\n",
      "Life is always longing for the way will not interfering not seek nothing unless we should rather than continuing to balance.\n",
      "Those who seek a liberated one that supreme love is ready for happiness only one loses joy and it.\n"
     ]
    }
   ],
   "source": [
    "# Creating a function that will print a desired number of generated quotes\n",
    "def repeat(times, f):\n",
    "    for i in range(times): f()\n",
    "    \n",
    "def do_v1():\n",
    "    print(duality())\n",
    "\n",
    "# Printing 5 generated quotes\n",
    "repeat(5, do_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of you are to go them should only feel Other in living .\n",
      "about it are to be you , you will attain it . go to be it , and yourself will Keep I .\n",
      "you see thought up not by he achieve then of I , of we achieve to let accomplished within your everyday things but possibilities . what He put and enlightenment still is positive to it . but if seeking demonic because the dissatisfaction at a world , the way dwell never find right No more .\n",
      "Miller activities of of we should establish out to The master behind our faith , us will find to be and use up of you .\n",
      "of a yourselves , body others . Zen from an effort and hold you my wood .\n"
     ]
    }
   ],
   "source": [
    "def do_v0():\n",
    "    print(chaos())\n",
    "\n",
    "# Printing 5 generated quotes\n",
    "repeat(5, do_v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1 results\n",
    "\n",
    "* Just do it.\n",
    "* In my friends you can get the fire you grow from it should scare you do drunk.\n",
    "* You.\n",
    "* I believe in the least for anything i believe in god from a man to exist.\n",
    "* Dont bother just take rest is too little one that you better.\n",
    "* If you can not what we know what you will remain constant.\n",
    "* What we are travelling more difficult than to forget is no greatness.\n",
    "* Anything you look for what you do not being yourself.\n",
    "* Let the wilderness of all else is still looking for us entirely happy because i told dismiss that can do something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: Trigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's... marginally better. Our ratio of \"potentially good\" generated quotes to \"gibberish quotes\" is still pretty awful. Let's see how a trigram model does instead.\n",
    "\n",
    "In the steps above, we took some risks with our tokens. Since we ended up turning our corpus back into a long string instead of a list, now we just have quotes after quotes that aren't necessarily related. This is a problem because we don't necessarily want trigrams that span from the end of one quote to the next. Those trigrams do not represent tokens that could follow each other in a text -- they are completely accidental.\n",
    "\n",
    "To address this, we added double end tokens for the trigrams: now, starting tokens look like `[END, END, word]` and end tokens like `[word, END, END]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding extra END tokens\n",
    "def add_extra_end_token(tokenized_document):\n",
    "    new_document = []\n",
    "    for token in tokenized_document:\n",
    "        new_document.append(token)\n",
    "        if token == \"END\":\n",
    "            new_document.append(\"END\")\n",
    "    return new_document\n",
    "\n",
    "def get_trigrams(document):\n",
    "    corpus = tokenize(document)\n",
    "    corpus = add_extra_end_token(corpus)\n",
    "    corpus_fd_bigram = nltk.FreqDist(nltk.bigrams([\"END\"] + corpus))\n",
    "    trigrams = nltk.trigrams([\"END\", \"END\"] + corpus)\n",
    "    trigrams_fd = nltk.FreqDist(trigrams)\n",
    "    results = {}\n",
    "    for trigram, trigram_frequency in trigrams_fd.items():\n",
    "        first_word, second_word, third_word = trigram\n",
    "        probability = (trigram_frequency) / (corpus_fd_bigram[(first_word, second_word)])\n",
    "        results[trigram] = probability\n",
    "    return results\n",
    "\n",
    "#get_trigrams(corpus)\n",
    "\n",
    "trigram_model = get_trigrams(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modified `get_mindful_v1` to be able to work with an N-gram model below, and `get_mindful_v2` is born:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mindful with Fulfillomatic version 3\n",
    "def open_your_third_eye():\n",
    "    \"\"\"\n",
    "    Three things cannot long be hidden: the sun, the moon, and the truth. ॐ\n",
    "    \n",
    "    (Trigram Model)\n",
    "    \"\"\"\n",
    "    sentence = \"\"\n",
    "    while len(sentence.split()) < 4:\n",
    "        sentence = get_sentence_with_ngram_model(3, trigram_model)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He plunges recklessly towards an irrational death.\n",
      "Be present above all else.\n",
      "When i am the infinite the vastness that is likely to hurt.\n",
      "Purity is something that is brought about by a calm mind and such peace of mind produces right values produce right thoughts produce right thoughts.\n",
      "Calmness in activity is true calmness.\n",
      "The more it tends to be.\n",
      "Those who worship do not let go of old judgments and opinions.\n",
      "Whether we like it or not change comes and goes comes and the entire sky are reflected in one dewdrop on the tops of mountains is the lesson.\n",
      "Anger ego jealousy are the slave to them.\n",
      "In the act of being open to all that.\n",
      "It will take quite a long time before you find out for yourself.\n",
      "Unless we die to ourselves we can open up our small mind.\n",
      "And when they played they really played.\n",
      "Not engaging in ignorance is wisdom.\n",
      "And when they played they really played.\n"
     ]
    }
   ],
   "source": [
    "# Print 5 generated sentences\n",
    "def do_v2():\n",
    "    print(open_your_third_eye())\n",
    "    \n",
    "repeat(15,do_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts\n",
    "\n",
    "Let's take a look at how some of our quotes are being put together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: \"It takes courage **to grow** sharper.\"\n",
    "\n",
    "Take: *\"The world is full of magic things, patiently waiting for our senses* **to grow** *sharper.\"*\n",
    "\n",
    "And: *\"It takes courage* **to grow** *up and become who you really are.\"*\n",
    "\n",
    "##### Get: It takes courage **to grow** sharper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we feed the model a bunch of Nietzsche quotes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Without music life would be a means to conceal oneself.\n",
    "* The noble soul reveres itself.\n",
    "* What is the struggle of opinions that is to preserve the distance which separates us from other men.\n",
    "* God is a rope over an abyss.\n",
    "* But there is also always some reason in madness.\n",
    "* We have forgotten are illusions.\n",
    "* Christianity is our taste no longer our reasons.\n",
    "* The end of a bad memory is too good.\n",
    "* The advantage of a strong faith is infallible.\n",
    "* There are two different types of people in the enemy’s staying alive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we feed the model a bunch of Zen quotes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When another person makes you rise to new heights no matter what.\n",
    "* The foolish reject what they crave.\n",
    "* The waters are in motion but the love of the need for complicated philosophy.\n",
    "* Wisdom is letting go of who you are.\n",
    "* So do the wise to resist pleasures but the moon does not last.\n",
    "* Nurture your mind you should burn yourself completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, but not too bad either! On average, and producing batches of 10, usually 4 of the quotes will be *pretty okay.*\n",
    "\n",
    "**Conclusion:** 40% of the time it works every time!\n",
    "\n",
    "### How do we know if our model is any good?\n",
    "\n",
    "Since we were generating sentences without a specifically pre-defined grammar, it was harder to justify using some of the metrics we learned this semester (F1 score, etc). Our methodology was looking at batches, cherry-picking the good ones, and seeing the ratios. This would vary every time we ran things but, it's safe to say that, except for a few gems, most of the quotes were pretty nonsensical.\n",
    "\n",
    "With this scenario, we thought about pushing things forward by using LSTM and implementing some kind of subject/verb agreement. We didn't do the latte but our attempt at the former is below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was, naturally, trying an RNN -- because why not? Anything larger than trigram as a long term dependency. Unfortunately, an RNN does not work practically in this situation. During the training, as the information loops it results in very large updates to neural network model weights, due to the accumulation of error gradients during an update. [This results in an unstable network.](https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47).\n",
    "\n",
    "Fortunately, LSTMs are a thing! A Long Short-Term Memory model outperforms the other models when we want our model to learn from long term dependencies. LSTM’s ability to forget, remember and update the information pushes it one step ahead of RNNs. To give this a shot, we followed and adapted the [How to Develop a Word-Level Neural Language Model and Use it to Generate Text](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/) tutorial to try to generate some mindfulness, and go one step further from opening our third eye.\n",
    "\n",
    "Right at the beginning of the article, the author says:\n",
    "\n",
    "*\"Neural network models are a preferred method for developing statistical language models because they can use a distributed representation where different words with similar meanings have similar representation and because they can use a large context of recently observed words when making predictions.\"*\n",
    "\n",
    "This is already alarming since our corpus (~614 quotes in the `inspirational_quotes.txt` file) isn't exactly big. The author also uses a sequence of the 50 previous words to predict the next using Plato's The Republic, which is way too much for us. We already thought we were at the limit with trigrams, but we need to try something bigger to give the LSTM a shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Loading packages\n",
    "import h5py  # Warning: this was a headache along with making sure all the HDF5 stuff was good too \n",
    "import keras\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code from the tutorial with a few adjustments for our data, like the size of the input sequences. You will note it is not exactly the same because there were some errors in the code itself that we corrected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and cleaning the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# Turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# Load document\n",
    "in_filename = 'training/inspirational_quotes.txt'\n",
    "doc = load_doc(in_filename)\n",
    "#print(doc[:200])\n",
    "\n",
    "# Clean document\n",
    "tokens = clean_doc(doc)\n",
    "\n",
    "\n",
    "# Organize into sequences of tokens\n",
    "length = 3 + 1     # Changed from 50+1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "\n",
    "# Save sequences to file\n",
    "out_filename = 'inspirational_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a look at some statistics about our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 14479\n",
      "Unique Tokens: 2319\n",
      "Total Sequences: 14475\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics about our quotes\n",
    "#print(tokens[:50])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and setting up the sequences and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3, 50)             115950    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 3, 100)            60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2319)              234219    \n",
      "=================================================================\n",
      "Total params: 501,069\n",
      "Trainable params: 501,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "in_filename = 'inspirational_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# Integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14475/14475 [==============================] - 8s 538us/step - loss: 6.6221 - acc: 0.0432\n",
      "Epoch 2/200\n",
      "14475/14475 [==============================] - 4s 270us/step - loss: 6.0116 - acc: 0.0462\n",
      "Epoch 3/200\n",
      "14475/14475 [==============================] - 4s 264us/step - loss: 5.9704 - acc: 0.0455\n",
      "Epoch 4/200\n",
      "14475/14475 [==============================] - 4s 265us/step - loss: 5.9494 - acc: 0.0435\n",
      "Epoch 5/200\n",
      "14475/14475 [==============================] - 4s 264us/step - loss: 5.9317 - acc: 0.0470\n",
      "Epoch 6/200\n",
      "14475/14475 [==============================] - 4s 265us/step - loss: 5.9034 - acc: 0.0463\n",
      "Epoch 7/200\n",
      "14475/14475 [==============================] - 4s 264us/step - loss: 5.8398 - acc: 0.0458\n",
      "Epoch 8/200\n",
      "14475/14475 [==============================] - 4s 267us/step - loss: 5.7573 - acc: 0.0529\n",
      "Epoch 9/200\n",
      "14475/14475 [==============================] - 4s 269us/step - loss: 5.6313 - acc: 0.0605\n",
      "Epoch 10/200\n",
      "14475/14475 [==============================] - 4s 265us/step - loss: 5.4905 - acc: 0.0694\n",
      "Epoch 11/200\n",
      "14475/14475 [==============================] - 4s 265us/step - loss: 5.3755 - acc: 0.0760\n",
      "Epoch 12/200\n",
      "14475/14475 [==============================] - 4s 266us/step - loss: 5.2815 - acc: 0.0792\n",
      "Epoch 13/200\n",
      "14475/14475 [==============================] - 4s 266us/step - loss: 5.1906 - acc: 0.0846\n",
      "Epoch 14/200\n",
      "14475/14475 [==============================] - 4s 265us/step - loss: 5.0989 - acc: 0.0908\n",
      "Epoch 15/200\n",
      "14475/14475 [==============================] - 4s 280us/step - loss: 5.0063 - acc: 0.0955\n",
      "Epoch 16/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 4.9109 - acc: 0.1059\n",
      "Epoch 17/200\n",
      "14475/14475 [==============================] - 4s 274us/step - loss: 4.8207 - acc: 0.1096\n",
      "Epoch 18/200\n",
      "14475/14475 [==============================] - 4s 289us/step - loss: 4.7324 - acc: 0.1159\n",
      "Epoch 19/200\n",
      "14475/14475 [==============================] - 4s 263us/step - loss: 4.6509 - acc: 0.1221\n",
      "Epoch 20/200\n",
      "14475/14475 [==============================] - 4s 262us/step - loss: 4.5608 - acc: 0.1275\n",
      "Epoch 21/200\n",
      "14475/14475 [==============================] - 4s 271us/step - loss: 4.4669 - acc: 0.1340\n",
      "Epoch 22/200\n",
      "14475/14475 [==============================] - 4s 280us/step - loss: 4.3779 - acc: 0.1391\n",
      "Epoch 23/200\n",
      "14475/14475 [==============================] - 4s 266us/step - loss: 4.2831 - acc: 0.1491\n",
      "Epoch 24/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 4.1899 - acc: 0.1533\n",
      "Epoch 25/200\n",
      "14475/14475 [==============================] - 4s 283us/step - loss: 4.0978 - acc: 0.1621\n",
      "Epoch 26/200\n",
      "14475/14475 [==============================] - 4s 299us/step - loss: 3.9999 - acc: 0.1686\n",
      "Epoch 27/200\n",
      "14475/14475 [==============================] - 4s 280us/step - loss: 3.9110 - acc: 0.1793\n",
      "Epoch 28/200\n",
      "14475/14475 [==============================] - 4s 272us/step - loss: 3.8164 - acc: 0.1876\n",
      "Epoch 29/200\n",
      "14475/14475 [==============================] - 4s 271us/step - loss: 3.7354 - acc: 0.1978\n",
      "Epoch 30/200\n",
      "14475/14475 [==============================] - 4s 268us/step - loss: 3.6510 - acc: 0.2038\n",
      "Epoch 31/200\n",
      "14475/14475 [==============================] - 4s 268us/step - loss: 3.5694 - acc: 0.2175\n",
      "Epoch 32/200\n",
      "14475/14475 [==============================] - 4s 267us/step - loss: 3.4823 - acc: 0.2296\n",
      "Epoch 33/200\n",
      "14475/14475 [==============================] - 4s 266us/step - loss: 3.4041 - acc: 0.2417\n",
      "Epoch 34/200\n",
      "14475/14475 [==============================] - 4s 268us/step - loss: 3.3269 - acc: 0.2512\n",
      "Epoch 35/200\n",
      "14475/14475 [==============================] - 4s 300us/step - loss: 3.2558 - acc: 0.2636\n",
      "Epoch 36/200\n",
      "14475/14475 [==============================] - 4s 288us/step - loss: 3.1970 - acc: 0.2776\n",
      "Epoch 37/200\n",
      "14475/14475 [==============================] - 4s 275us/step - loss: 3.1100 - acc: 0.2877\n",
      "Epoch 38/200\n",
      "14475/14475 [==============================] - 4s 261us/step - loss: 3.0328 - acc: 0.3059\n",
      "Epoch 39/200\n",
      "14475/14475 [==============================] - 4s 264us/step - loss: 2.9649 - acc: 0.3175\n",
      "Epoch 40/200\n",
      "14475/14475 [==============================] - 4s 267us/step - loss: 2.8960 - acc: 0.3275\n",
      "Epoch 41/200\n",
      "14475/14475 [==============================] - 4s 260us/step - loss: 2.8322 - acc: 0.3384\n",
      "Epoch 42/200\n",
      "14475/14475 [==============================] - 4s 262us/step - loss: 2.7758 - acc: 0.3497\n",
      "Epoch 43/200\n",
      "14475/14475 [==============================] - 4s 257us/step - loss: 2.7091 - acc: 0.3645\n",
      "Epoch 44/200\n",
      "14475/14475 [==============================] - 4s 258us/step - loss: 2.6485 - acc: 0.3774\n",
      "Epoch 45/200\n",
      "14475/14475 [==============================] - 4s 265us/step - loss: 2.5841 - acc: 0.3889\n",
      "Epoch 46/200\n",
      "14475/14475 [==============================] - 4s 259us/step - loss: 2.5223 - acc: 0.4000\n",
      "Epoch 47/200\n",
      "14475/14475 [==============================] - 4s 273us/step - loss: 2.4715 - acc: 0.4122\n",
      "Epoch 48/200\n",
      "14475/14475 [==============================] - 4s 300us/step - loss: 2.4183 - acc: 0.4188\n",
      "Epoch 49/200\n",
      "14475/14475 [==============================] - 4s 272us/step - loss: 2.3563 - acc: 0.4388\n",
      "Epoch 50/200\n",
      "14475/14475 [==============================] - 4s 273us/step - loss: 2.3111 - acc: 0.4464\n",
      "Epoch 51/200\n",
      "14475/14475 [==============================] - 4s 260us/step - loss: 2.2700 - acc: 0.4534\n",
      "Epoch 52/200\n",
      "14475/14475 [==============================] - 4s 270us/step - loss: 2.2176 - acc: 0.4645\n",
      "Epoch 53/200\n",
      "14475/14475 [==============================] - 4s 263us/step - loss: 2.1639 - acc: 0.4762\n",
      "Epoch 54/200\n",
      "14475/14475 [==============================] - 4s 262us/step - loss: 2.1218 - acc: 0.4891\n",
      "Epoch 55/200\n",
      "14475/14475 [==============================] - 4s 266us/step - loss: 2.0728 - acc: 0.4961\n",
      "Epoch 56/200\n",
      "14475/14475 [==============================] - 4s 268us/step - loss: 2.0286 - acc: 0.5099\n",
      "Epoch 57/200\n",
      "14475/14475 [==============================] - 4s 267us/step - loss: 1.9943 - acc: 0.5124\n",
      "Epoch 58/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 1.9512 - acc: 0.5232\n",
      "Epoch 59/200\n",
      "14475/14475 [==============================] - 4s 266us/step - loss: 1.9043 - acc: 0.5348\n",
      "Epoch 60/200\n",
      "14475/14475 [==============================] - 4s 268us/step - loss: 1.8740 - acc: 0.5362\n",
      "Epoch 61/200\n",
      "14475/14475 [==============================] - 4s 264us/step - loss: 1.8421 - acc: 0.5467\n",
      "Epoch 62/200\n",
      "14475/14475 [==============================] - 4s 267us/step - loss: 1.7971 - acc: 0.5569\n",
      "Epoch 63/200\n",
      "14475/14475 [==============================] - 4s 267us/step - loss: 1.7732 - acc: 0.5630\n",
      "Epoch 64/200\n",
      "14475/14475 [==============================] - 4s 268us/step - loss: 1.7299 - acc: 0.5731\n",
      "Epoch 65/200\n",
      "14475/14475 [==============================] - 4s 269us/step - loss: 1.7004 - acc: 0.5766\n",
      "Epoch 66/200\n",
      "14475/14475 [==============================] - 4s 287us/step - loss: 1.6927 - acc: 0.5784\n",
      "Epoch 67/200\n",
      "14475/14475 [==============================] - 4s 273us/step - loss: 1.6358 - acc: 0.5922\n",
      "Epoch 68/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 1.6186 - acc: 0.5947\n",
      "Epoch 69/200\n",
      "14475/14475 [==============================] - 4s 274us/step - loss: 1.5803 - acc: 0.6038\n",
      "Epoch 70/200\n",
      "14475/14475 [==============================] - 4s 273us/step - loss: 1.5472 - acc: 0.6124\n",
      "Epoch 71/200\n",
      "14475/14475 [==============================] - 4s 274us/step - loss: 1.5207 - acc: 0.6198\n",
      "Epoch 72/200\n",
      "14475/14475 [==============================] - 4s 273us/step - loss: 1.4835 - acc: 0.6272\n",
      "Epoch 73/200\n",
      "14475/14475 [==============================] - 4s 275us/step - loss: 1.4554 - acc: 0.6334\n",
      "Epoch 74/200\n",
      "14475/14475 [==============================] - 4s 276us/step - loss: 1.4284 - acc: 0.6388\n",
      "Epoch 75/200\n",
      "14475/14475 [==============================] - 4s 275us/step - loss: 1.4021 - acc: 0.6462\n",
      "Epoch 76/200\n",
      "14475/14475 [==============================] - 4s 275us/step - loss: 1.3787 - acc: 0.6511\n",
      "Epoch 77/200\n",
      "14475/14475 [==============================] - 4s 276us/step - loss: 1.3608 - acc: 0.6569\n",
      "Epoch 78/200\n",
      "14475/14475 [==============================] - 4s 276us/step - loss: 1.3234 - acc: 0.6644\n",
      "Epoch 79/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 1.3040 - acc: 0.6682\n",
      "Epoch 80/200\n",
      "14475/14475 [==============================] - 4s 280us/step - loss: 1.2908 - acc: 0.6705\n",
      "Epoch 81/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 1.2587 - acc: 0.6799\n",
      "Epoch 82/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 1.2426 - acc: 0.6809\n",
      "Epoch 83/200\n",
      "14475/14475 [==============================] - 4s 276us/step - loss: 1.2207 - acc: 0.6871\n",
      "Epoch 84/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 1.1962 - acc: 0.6913\n",
      "Epoch 85/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 1.1727 - acc: 0.7000\n",
      "Epoch 86/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 1.1550 - acc: 0.7007\n",
      "Epoch 87/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 1.1185 - acc: 0.7125\n",
      "Epoch 88/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 1.1016 - acc: 0.7161\n",
      "Epoch 89/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 1.0885 - acc: 0.7179\n",
      "Epoch 90/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 1.0807 - acc: 0.7177\n",
      "Epoch 91/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 1.0512 - acc: 0.7283\n",
      "Epoch 92/200\n",
      "14475/14475 [==============================] - 4s 280us/step - loss: 1.0304 - acc: 0.7317\n",
      "Epoch 93/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 1.0081 - acc: 0.7345\n",
      "Epoch 94/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.9903 - acc: 0.7426\n",
      "Epoch 95/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.9764 - acc: 0.7469\n",
      "Epoch 96/200\n",
      "14475/14475 [==============================] - 4s 280us/step - loss: 0.9522 - acc: 0.7514\n",
      "Epoch 97/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.9429 - acc: 0.7513\n",
      "Epoch 98/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.9261 - acc: 0.7552\n",
      "Epoch 99/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.9125 - acc: 0.7606\n",
      "Epoch 100/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.9035 - acc: 0.7611\n",
      "Epoch 101/200\n",
      "14475/14475 [==============================] - 4s 280us/step - loss: 0.8843 - acc: 0.7665\n",
      "Epoch 102/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.8535 - acc: 0.7739\n",
      "Epoch 103/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.8406 - acc: 0.7767\n",
      "Epoch 104/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.8326 - acc: 0.7787\n",
      "Epoch 105/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.8130 - acc: 0.7847\n",
      "Epoch 106/200\n",
      "14475/14475 [==============================] - 4s 289us/step - loss: 0.8113 - acc: 0.7845\n",
      "Epoch 107/200\n",
      "14475/14475 [==============================] - 4s 284us/step - loss: 0.7967 - acc: 0.7897\n",
      "Epoch 108/200\n",
      "14475/14475 [==============================] - 4s 282us/step - loss: 0.7773 - acc: 0.7937\n",
      "Epoch 109/200\n",
      "14475/14475 [==============================] - 4s 283us/step - loss: 0.7724 - acc: 0.7937\n",
      "Epoch 110/200\n",
      "14475/14475 [==============================] - 4s 282us/step - loss: 0.7555 - acc: 0.7975\n",
      "Epoch 111/200\n",
      "14475/14475 [==============================] - 4s 283us/step - loss: 0.7398 - acc: 0.7994\n",
      "Epoch 112/200\n",
      "14475/14475 [==============================] - 4s 283us/step - loss: 0.7329 - acc: 0.8053\n",
      "Epoch 113/200\n",
      "14475/14475 [==============================] - 4s 284us/step - loss: 0.7173 - acc: 0.8060\n",
      "Epoch 114/200\n",
      "14475/14475 [==============================] - 4s 288us/step - loss: 0.6969 - acc: 0.8127\n",
      "Epoch 115/200\n",
      "14475/14475 [==============================] - 4s 282us/step - loss: 0.6699 - acc: 0.8203\n",
      "Epoch 116/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.6606 - acc: 0.8239\n",
      "Epoch 117/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.6432 - acc: 0.8293\n",
      "Epoch 118/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.6288 - acc: 0.8327\n",
      "Epoch 119/200\n",
      "14475/14475 [==============================] - 4s 275us/step - loss: 0.6263 - acc: 0.8308\n",
      "Epoch 120/200\n",
      "14475/14475 [==============================] - 4s 280us/step - loss: 0.6253 - acc: 0.8294\n",
      "Epoch 121/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.6211 - acc: 0.8328\n",
      "Epoch 122/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.6146 - acc: 0.8323\n",
      "Epoch 123/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.5910 - acc: 0.8388\n",
      "Epoch 124/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.5793 - acc: 0.8417\n",
      "Epoch 125/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.5785 - acc: 0.8422\n",
      "Epoch 126/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.5633 - acc: 0.8477\n",
      "Epoch 127/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.5559 - acc: 0.8476\n",
      "Epoch 128/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.5463 - acc: 0.8515\n",
      "Epoch 129/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.5427 - acc: 0.8494\n",
      "Epoch 130/200\n",
      "14475/14475 [==============================] - 4s 284us/step - loss: 0.5463 - acc: 0.8479\n",
      "Epoch 131/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.5323 - acc: 0.8537\n",
      "Epoch 132/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.5241 - acc: 0.8537\n",
      "Epoch 133/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.5114 - acc: 0.8577\n",
      "Epoch 134/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.4926 - acc: 0.8613\n",
      "Epoch 135/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.4793 - acc: 0.8647\n",
      "Epoch 136/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.4801 - acc: 0.8664\n",
      "Epoch 137/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.4641 - acc: 0.8697\n",
      "Epoch 138/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.4611 - acc: 0.8713\n",
      "Epoch 139/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.4573 - acc: 0.8719\n",
      "Epoch 140/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.4477 - acc: 0.8716\n",
      "Epoch 141/200\n",
      "14475/14475 [==============================] - 4s 298us/step - loss: 0.4516 - acc: 0.8726\n",
      "Epoch 142/200\n",
      "14475/14475 [==============================] - 4s 297us/step - loss: 0.4513 - acc: 0.8690\n",
      "Epoch 143/200\n",
      "14475/14475 [==============================] - 4s 297us/step - loss: 0.4339 - acc: 0.8771\n",
      "Epoch 144/200\n",
      "14475/14475 [==============================] - 4s 295us/step - loss: 0.4275 - acc: 0.8779\n",
      "Epoch 145/200\n",
      "14475/14475 [==============================] - 4s 291us/step - loss: 0.4343 - acc: 0.8740\n",
      "Epoch 146/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.4184 - acc: 0.8810\n",
      "Epoch 147/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.4291 - acc: 0.8793\n",
      "Epoch 148/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.4002 - acc: 0.8843\n",
      "Epoch 149/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.3996 - acc: 0.8855\n",
      "Epoch 150/200\n",
      "14475/14475 [==============================] - 4s 282us/step - loss: 0.4014 - acc: 0.8824\n",
      "Epoch 151/200\n",
      "14475/14475 [==============================] - 4s 276us/step - loss: 0.3945 - acc: 0.8840\n",
      "Epoch 152/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.3871 - acc: 0.8855\n",
      "Epoch 153/200\n",
      "14475/14475 [==============================] - 4s 276us/step - loss: 0.3904 - acc: 0.8855\n",
      "Epoch 154/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.3700 - acc: 0.8917\n",
      "Epoch 155/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.3597 - acc: 0.8936\n",
      "Epoch 156/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.3724 - acc: 0.8893\n",
      "Epoch 157/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.3851 - acc: 0.8875\n",
      "Epoch 158/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.3747 - acc: 0.8880\n",
      "Epoch 159/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.3732 - acc: 0.8904\n",
      "Epoch 160/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.3612 - acc: 0.8921\n",
      "Epoch 161/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.3537 - acc: 0.8926\n",
      "Epoch 162/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.3439 - acc: 0.8953\n",
      "Epoch 163/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.3344 - acc: 0.8978\n",
      "Epoch 164/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.3291 - acc: 0.9007\n",
      "Epoch 165/200\n",
      "14475/14475 [==============================] - 4s 278us/step - loss: 0.3210 - acc: 0.9004\n",
      "Epoch 166/200\n",
      "14475/14475 [==============================] - 4s 280us/step - loss: 0.3139 - acc: 0.9025\n",
      "Epoch 167/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.3098 - acc: 0.9049\n",
      "Epoch 168/200\n",
      "14475/14475 [==============================] - 4s 296us/step - loss: 0.3186 - acc: 0.9022\n",
      "Epoch 169/200\n",
      "14475/14475 [==============================] - 4s 302us/step - loss: 0.3034 - acc: 0.9042\n",
      "Epoch 170/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.3067 - acc: 0.9054\n",
      "Epoch 171/200\n",
      "14475/14475 [==============================] - 4s 276us/step - loss: 0.3064 - acc: 0.9033\n",
      "Epoch 172/200\n",
      "14475/14475 [==============================] - 4s 282us/step - loss: 0.3107 - acc: 0.9045\n",
      "Epoch 173/200\n",
      "14475/14475 [==============================] - 4s 283us/step - loss: 0.3124 - acc: 0.9007\n",
      "Epoch 174/200\n",
      "14475/14475 [==============================] - 4s 283us/step - loss: 0.3082 - acc: 0.9024\n",
      "Epoch 175/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.3129 - acc: 0.9014\n",
      "Epoch 176/200\n",
      "14475/14475 [==============================] - 4s 279us/step - loss: 0.3082 - acc: 0.9033\n",
      "Epoch 177/200\n",
      "14475/14475 [==============================] - 4s 286us/step - loss: 0.3027 - acc: 0.9061\n",
      "Epoch 178/200\n",
      "14475/14475 [==============================] - 4s 295us/step - loss: 0.3290 - acc: 0.8969\n",
      "Epoch 179/200\n",
      "14475/14475 [==============================] - 4s 288us/step - loss: 0.3437 - acc: 0.8915\n",
      "Epoch 180/200\n",
      "14475/14475 [==============================] - 4s 286us/step - loss: 0.3125 - acc: 0.8991\n",
      "Epoch 181/200\n",
      "14475/14475 [==============================] - 4s 295us/step - loss: 0.2938 - acc: 0.9050\n",
      "Epoch 182/200\n",
      "14475/14475 [==============================] - 4s 281us/step - loss: 0.2843 - acc: 0.9082\n",
      "Epoch 183/200\n",
      "14475/14475 [==============================] - 4s 290us/step - loss: 0.2865 - acc: 0.9058\n",
      "Epoch 184/200\n",
      "14475/14475 [==============================] - 4s 289us/step - loss: 0.2902 - acc: 0.9058\n",
      "Epoch 185/200\n",
      "14475/14475 [==============================] - 5s 320us/step - loss: 0.2905 - acc: 0.9072\n",
      "Epoch 186/200\n",
      "14475/14475 [==============================] - 4s 305us/step - loss: 0.2793 - acc: 0.9078\n",
      "Epoch 187/200\n",
      "14475/14475 [==============================] - 4s 310us/step - loss: 0.2732 - acc: 0.9094\n",
      "Epoch 188/200\n",
      "14475/14475 [==============================] - 5s 329us/step - loss: 0.2696 - acc: 0.9101\n",
      "Epoch 189/200\n",
      "14475/14475 [==============================] - 5s 322us/step - loss: 0.2702 - acc: 0.9099\n",
      "Epoch 190/200\n",
      "14475/14475 [==============================] - 4s 285us/step - loss: 0.2721 - acc: 0.9103\n",
      "Epoch 191/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.2650 - acc: 0.9098\n",
      "Epoch 192/200\n",
      "14475/14475 [==============================] - 4s 277us/step - loss: 0.2688 - acc: 0.9093\n",
      "Epoch 193/200\n",
      "14475/14475 [==============================] - 4s 301us/step - loss: 0.2658 - acc: 0.9130\n",
      "Epoch 194/200\n",
      "14475/14475 [==============================] - 4s 295us/step - loss: 0.2659 - acc: 0.9120\n",
      "Epoch 195/200\n",
      "14475/14475 [==============================] - 4s 285us/step - loss: 0.2647 - acc: 0.9083\n",
      "Epoch 196/200\n",
      "14475/14475 [==============================] - 4s 284us/step - loss: 0.2750 - acc: 0.9077\n",
      "Epoch 197/200\n",
      "14475/14475 [==============================] - 4s 284us/step - loss: 0.2790 - acc: 0.9087\n",
      "Epoch 198/200\n",
      "14475/14475 [==============================] - 4s 297us/step - loss: 0.2807 - acc: 0.9070\n",
      "Epoch 199/200\n",
      "14475/14475 [==============================] - 4s 284us/step - loss: 0.2754 - acc: 0.9074\n",
      "Epoch 200/200\n",
      "14475/14475 [==============================] - 4s 282us/step - loss: 0.2707 - acc: 0.9083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x213cc604160>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had some issues installing `h5py` and having current `HDF5` on Windows so the two lines commented on the next cell address an error that comes up in the Keras package about these two dependencies when you try to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# reload(keras.models)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calligraphy painting music or\n",
      "\n",
      "whatever you think you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'inspirational_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for each word\n",
    "yhat = model.predict_classes(np.array(encoded)[:, 1:], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "        out_word = word\n",
    "        break\n",
    "        \n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot do interfere with what you can do that you can at all the times you can to all the people you can as long as you keep yourself centered the time is always right to do what is essential what the logos of a social being requires and in the requisite way which brings a double satisfaction to do less better because most of what we say and do is not doing not being able to choose which way to go and for what purpose to make the right things happen what really distinguishes this generation in all countries from earlier generations is its determination to act its joy in action the assurance of being able to choose which way to go and for what purpose to make the right things happen what really distinguishes this generation in all countries from earlier generations is its determination to act its joy in action the assurance of being able to choose which way to go and for what purpose to make the right things happen what really distinguishes this generation in all countries from earlier generations is its determination to act its joy in action the assurance of being able to\n"
     ]
    }
   ],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "                # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 200)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLG](https://supportivedivorcesolutions.com/wp-content/uploads/2017/03/iStock-468140568.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
