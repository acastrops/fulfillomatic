{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fulfillomatic\n",
    "\n",
    "##### Adriana Souza, Roger Filmyer\n",
    "\n",
    "##### This notebook will be finished/cleaned by Thursday, Dec 6th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLG](http://www.pngall.com/wp-content/uploads/2016/07/Meditation-Transparent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the file to use\n",
    "file = 'Corpus/quotes.txt'\n",
    "\n",
    "# Storing quotes from file in a list\n",
    "with open(file) as opened_file: \n",
    "    lists = opened_file.read().splitlines()\n",
    "    quotes = []\n",
    "    for line in lists:\n",
    "        quotes.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0: Uniform Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'day must be and be . God to all despair of pushes .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize\n",
    "tokenized_corpus = []\n",
    "for quote in quotes:\n",
    "    tokenized_quote = nltk.tokenize.word_tokenize(quote)\n",
    "    tagged_quote = nltk.pos_tag(tokenized_quote)\n",
    "    tokenized_corpus.append(tagged_quote)\n",
    "\n",
    "# Set up the language model\n",
    "parts_of_speech = defaultdict(list)\n",
    "sentence_structures = []\n",
    "for quote in tokenized_corpus:\n",
    "    sentence_structure = []\n",
    "    for word, pos in quote:\n",
    "        parts_of_speech[pos].append(word)\n",
    "        sentence_structure.append(pos)\n",
    "    sentence_structures.append(sentence_structure)\n",
    "\n",
    "# Generate an example sentence\n",
    "def get_mindful_v0() -> str:\n",
    "    \"\"\"\n",
    "    Generate an inspirational sentence. \n",
    "    \n",
    "    Ensure that you are in the proper state of mind before running. ॐ\n",
    "    \"\"\"\n",
    "    sentence_skeleton = random.choice(sentence_structures)\n",
    "\n",
    "    reconstituted_sentence = []\n",
    "    for part_of_speech in sentence_skeleton:\n",
    "        new_word = random.choice(parts_of_speech[part_of_speech])\n",
    "        reconstituted_sentence.append(new_word)\n",
    "\n",
    "    return \" \".join(reconstituted_sentence)\n",
    "\n",
    "# Output\n",
    "get_mindful_v0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 0 results\n",
    "\n",
    "* your ready Speak begins when you can hear you not and never .\n",
    "* in I think busy forwards of coffee , forever it . in you will live aware library you , make your education .\n",
    "* without t denies my anything that bulk , yourself can once call You .\n",
    "* as I are dreams to don grief , never the sun is to tolerate able you .\n",
    "* all valuable choice is than the painful comfort , it can keep imprisoned believe only not that you ’ you ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1: Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that worked great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning list into string\n",
    "corpus = \"\"\n",
    "string\n",
    "for word in quotes:\n",
    "    word = word.lower()\n",
    "    word = word.replace('.', ' END ')\n",
    "    #words = words.translate(str.maketrans, string.punctuation)\n",
    "    table = str.maketrans('','', string.punctuation + '…”“–')      # Remove punctuation\n",
    "    word = word.translate(table)\n",
    "    corpus = corpus + word  \n",
    "    \n",
    "def tokenize(input_string):\n",
    "    return input_string.split()\n",
    "\n",
    "def get_bigrams(corpus):\n",
    "    corpus_fd_unigram = nltk.FreqDist(tokenize(corpus))\n",
    "    total = sum([1 + i[1] for i in corpus_fd_unigram.items()])\n",
    "    bigrams = nltk.bigrams(['END'] + tokenize(corpus))\n",
    "    bigrams_fd = nltk.FreqDist(bigrams)\n",
    "    results = {}\n",
    "    for bigram, bigram_frequency in bigrams_fd.items():\n",
    "        first_word, second_word = bigram\n",
    "        probability = (bigram_frequency / corpus_fd_unigram[first_word])    \n",
    "        results[bigram] = probability\n",
    "    return results\n",
    "\n",
    "#bigrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_fd_unigram = nltk.FreqDist(tokenize(corpus))\n",
    "# total = sum([1 + i[1] for i in corpus_fd_unigram.items()])\n",
    "# bigrams = nltk.bigrams(['END'] + tokenize(corpus))\n",
    "# bigrams_fd = nltk.FreqDist(bigrams)\n",
    "# results = {}\n",
    "# for bigram, bigram_frequency in bigrams_fd.items():\n",
    "#     first_word, second_word = bigram\n",
    "#     probability = (bigram_frequency / corpus_fd_unigram[first_word])\n",
    "\n",
    "#     results[bigram] = probability\n",
    "#     #print('P(%s|%s)=' %(key[0],key[1]), '%s/%s' %((value+1),(corpus_fd[key[0]]+4)))\n",
    "\n",
    "# total = 0\n",
    "# word = 'END'\n",
    "\n",
    "# for key, value in bigrams_fd.items():\n",
    "#     if key[0] == word:\n",
    "#         print(key, value)\n",
    "#         total += value\n",
    "\n",
    "# print(total)\n",
    "# print(corpus_fd_unigram[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An artist is the present.\n",
      "Live our mistakes are.\n",
      "To reach out how to laugh you judge each honest worker.\n",
      "Hope is only walls.\n",
      "When you will find yourself everyone and buddha and reflect.\n",
      "Dont think like those around you do i can without explaining yourself think you have accomplished by the highest most powerful you plan b.\n",
      "You will change.\n",
      "Paths are going you will become as a man or get closer to be with a powerful you laughed often the most often lead to have only be the only to yourself think you will be the day by being superior to create than the beginning is best and what you will remain tight in life.\n",
      "We do.\n",
      "It.\n"
     ]
    }
   ],
   "source": [
    "bigram_model = get_bigrams(corpus)\n",
    "\n",
    "def get_mindful_v1():\n",
    "    \"\"\"\n",
    "    You must only concentrate on the next step, the next breath, \n",
    "    the next stroke of the broom, and the next, and the next. Nothing else.\n",
    "    ॐ\n",
    "    \n",
    "    (Bigram Model)\n",
    "    \"\"\"\n",
    "    words_in_sentence = ['END']\n",
    "    second_word = None\n",
    "    while second_word != 'END':\n",
    "        first_word = words_in_sentence[-1]\n",
    "        matching_bigrams = [bigram for bigram in bigram_model.keys() if bigram[0] == first_word]\n",
    "        bigram_probabilities = [bigram_model[bigram] for bigram in matching_bigrams]\n",
    "        total_probability = sum(bigram_probabilities)\n",
    "        second_word = np.random.choice(\n",
    "                        a=[second for first, second in matching_bigrams],\n",
    "                        p=[p for p in bigram_probabilities])\n",
    "        words_in_sentence.append(second_word)\n",
    "    words_in_sentence = words_in_sentence[1:-1]\n",
    "    # capitalize first letter of first word\n",
    "    if len(words_in_sentence) > 0:\n",
    "        first_word = words_in_sentence[0]\n",
    "        first_word = first_word[0].upper() + first_word[1:]\n",
    "        words_in_sentence[0] = first_word\n",
    "        sentence = \" \".join(words_in_sentence) + '.'\n",
    "    else:\n",
    "        sentence = get_mindful_v1()\n",
    "    return sentence\n",
    "        \n",
    "# Print it a\n",
    "def repeat(times, f):\n",
    "    for i in range(times): f()\n",
    "        \n",
    "def do_v1():\n",
    "    print(get_mindful_v1())\n",
    "    \n",
    "repeat(10, do_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who treats you can chain me in poverty.\n",
      "Life is to recognize that best.\n",
      "Every morning you can turn the powers of theory.\n",
      "There and afterward some people you will have no matter.\n",
      "He who can do not lead.\n",
      "The most powerful than any other time.\n",
      "If you can get better than ignorance than fade and not kill us stronger than ignorance than live so hard duty to another is a process.\n",
      "No man on silence.\n",
      "You will start right to be of stories it is that we enjoy constitutes our life is a dream more painful than the best kiss is enough.\n",
      "Man of rebellion.\n"
     ]
    }
   ],
   "source": [
    "repeat(10, do_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1 results\n",
    "\n",
    "* Be prepared is just keep looking for yourself everyone and nobody but you have no time for ourselves through it for your dreams begin it you.\n",
    "* Say no simplicity goodness much are to get in themselves.\n",
    "* Do not afraid of your contemporaries or are the measure of life is better advice than love you have been exchanged a different you can overcome them.\n",
    "* Dont look fear mistakes.\n",
    "* Education in humanity.\n",
    "* Learn creating is they wish to where I have no worries.\n",
    "* One less effort the other way to influence god will be man but yourself.\n",
    "\n",
    "* Dreaming we change your rope tie a valuable gift and you have was clever so on the best to represent not to be thought foolish and loving silence and besides the modern world is right words wait for it.\n",
    "* If you can as they have the fuck you a lot of my schooling interfere with my chief duty to reach out my life are ordinary.\n",
    "* There is enough to continually fear the gift of your face your temper or diplomas but what you want to be better than ignorance.\n",
    "* If you can smell it is all the most of men who you have to be.\n",
    "* Excellence then is not come this far.\n",
    "* Fail is he is in awareness.\n",
    "* Never get better.\n",
    "* You are the time to be calm and then is mankind.\n",
    "* That your mind is not with complete freedom.\n",
    "* Let your own enlightenment.\n",
    "\n",
    "* Just do it.\n",
    "* In my friends you can get the fire you grow from it should scare you do drunk.\n",
    "* You.\n",
    "* I believe in the least for anything i believe in god from a man to exist.\n",
    "* If you see.\n",
    "* Dont bother just take rest is too little one that you better.\n",
    "* If you can not what we know what you will remain constant.\n",
    "* What we are travelling more difficult than to forget is no greatness.\n",
    "* Anything you look for what you do not being yourself.\n",
    "* Let the wilderness of all else is still looking for us entirely happy because i told dismiss that can do something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: Trigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(...) We don't want trigrams that span from the end of one file to the next. Such trigrams do not represent tokens that could follow each other in a text-- they are completely accidental.\n",
    "\n",
    "(...)\n",
    "\n",
    "We added double end tokens for the trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding extra END tokens\n",
    "def add_extra_end_token(tokenized_document):\n",
    "    new_document = []\n",
    "    for token in tokenized_document:\n",
    "        new_document.append(token)\n",
    "        if token == \"END\":\n",
    "            new_document.append(\"END\")\n",
    "    return new_document\n",
    "\n",
    "def get_trigrams(document):\n",
    "    corpus = tokenize(document)\n",
    "    corpus = add_extra_end_token(corpus)\n",
    "    corpus_fd_bigram = nltk.FreqDist(nltk.bigrams([\"END\"] + corpus))\n",
    "    trigrams = nltk.trigrams([\"END\", \"END\"] + corpus)\n",
    "    trigrams_fd = nltk.FreqDist(trigrams)\n",
    "    results = {}\n",
    "    for trigram, trigram_frequency in trigrams_fd.items():\n",
    "        first_word, second_word, third_word = trigram\n",
    "        probability = (trigram_frequency) / (corpus_fd_bigram[(first_word, second_word)])\n",
    "        results[trigram] = probability\n",
    "    return results\n",
    "\n",
    "#get_trigrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[trigram for trigram in trigram_model.keys() if trigram[1] == 'doth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will not rise to the old brag of my friends.\n",
      "It is all you can calculate the worth of a subject.\n",
      "You will keep forever is the thing with stylenow that is not important.\n",
      "Hope is to love love yourself first.\n",
      "And that is greater than yourself the more sand that has been through so much but is still standing.\n",
      "Too much sanity may be right where you are.\n",
      "Happiness consists in realizing it is perfectly okay and absolutely necessary to shut down kick back and do not want to be thought foolish and stupid.\n",
      "Always go with the same weapons of reason which today arm you against the present.\n",
      "What truly horrible lives they must change if they are the first time.\n",
      "It is because they have accomplished it is that they have been to yourself.\n"
     ]
    }
   ],
   "source": [
    "trigram_model = get_trigrams(corpus)\n",
    "\n",
    "def get_sentence_with_ngram_model(num_words, model):\n",
    "    words_in_sentence = ['END' for i in range(0, num_words - 1)] # pad the start of the sentence with 'END' tokens\n",
    "    final_word = None\n",
    "    while final_word != 'END':        \n",
    "        initial_n_gram_words = words_in_sentence[-(num_words - 1):]\n",
    "        matching_n_gram_keys = []\n",
    "        for n_gram in model.keys():\n",
    "            words_to_match = zip(n_gram, initial_n_gram_words)\n",
    "            if all(a == b for a, b in words_to_match):\n",
    "                matching_n_gram_keys.append(n_gram)        \n",
    "        n_gram_probabilities = [model[n_gram] for n_gram in matching_n_gram_keys]        \n",
    "        total_probability = sum(n_gram_probabilities)                \n",
    "        final_word = np.random.choice(\n",
    "                        a=[n_gram[-1] for n_gram in matching_n_gram_keys],\n",
    "                        p=[p for p in n_gram_probabilities])\n",
    "        words_in_sentence.append(final_word)\n",
    "    words_in_sentence = words_in_sentence[(num_words - 1): -1]\n",
    "    # capitalize first letter of first word\n",
    "    if len(words_in_sentence) > 0:\n",
    "        first_word = words_in_sentence[0]\n",
    "        first_word = first_word[0].upper() + first_word[1:]\n",
    "        words_in_sentence[0] = first_word\n",
    "        sentence = \" \".join(words_in_sentence) + '.'\n",
    "    else:\n",
    "        sentence = get_sentence_with_ngram_model(num_words, model)\n",
    "    return sentence\n",
    "\n",
    "def get_mindful_v2():\n",
    "    \"\"\"\n",
    "    Three things cannot long be hidden: the sun, the moon, and the truth. ॐ\n",
    "    \n",
    "    (Trigram Model)\n",
    "    \"\"\"\n",
    "    sentence = \"\"\n",
    "    while len(sentence.split()) < 4:\n",
    "        sentence = get_sentence_with_ngram_model(3, trigram_model)\n",
    "    return sentence\n",
    "    \n",
    "        \n",
    "# Print a bunch of generated sentences\n",
    "def repeat(times, f):\n",
    "    for i in range(times): f()\n",
    "        \n",
    "def do_v1():\n",
    "    print(get_mindful_v2())\n",
    "    \n",
    "repeat(10, do_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "\n",
    "Take:\n",
    "\n",
    "The world is full of magic things, patiently waiting for our senses to grow sharper. \n",
    "\n",
    "And:\n",
    "\n",
    "It takes courage to grow up and become who you really are. \n",
    "\n",
    "Get:\n",
    "\n",
    "* It takes courage to grow sharper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLG](https://supportivedivorcesolutions.com/wp-content/uploads/2017/03/iStock-468140568.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
